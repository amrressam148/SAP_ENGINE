import streamlit as st
# Ø§Ù…Ø³Ø­ Ø¬ÙˆØ¬Ù„
from langchain_ollama.llms import OllamaLLM # (Ø¯Ù‡ Ø§Ù„Ø¬Ø¯ÙŠØ¯)
from langchain_core.prompts import ChatPromptTemplate
from vector import retriever # (Ø¯Ù‡ Ù‡ÙŠØ³ØªØ¯Ø¹ÙŠ Ø§Ù„Ù€ retriever Ø§Ù„Ù…Ø­Ù„ÙŠ)
# Ø§Ù…Ø³Ø­ os

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¸Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„) ---
st.set_page_config(page_title="Signavio Sage", page_icon="ğŸ¤–")

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠ) ---
@st.cache_resource
def load_model_chain():
    # (Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ø®ÙÙŠÙ)
    model = OllamaLLM(model="mistral:7b") 

    # (Ù†ÙØ³ Ø§Ù„Ù€ Prompt Ø¨ØªØ§Ø¹Ùƒ Ø¨Ø§Ù„Ø¸Ø¨Ø·)
    template = """
    You are 'Signavio Sage', ...
    ... (ÙƒÙ„ Ø§Ù„Ù€ template Ø¨ØªØ§Ø¹Ùƒ) ...
    """

    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | model
    return chain

try:
    chain = load_model_chain()
    st.success("Ollama model (mistral:7b) loaded successfully!") # (ØºÙŠØ±Ù†Ø§ Ø§Ù„Ø±Ø³Ø§Ù„Ø©)
except Exception as e:
    st.error(f"Error loading model: {e}")
    st.stop()

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù€ GUI Ø¨ØªØ§Ø¹Ùƒ - Ø²ÙŠ Ù…Ø§ Ù‡Ùˆ) ---
st.title("ğŸ¤– Signavio Sage Assistant")
st.info("Ask me anything... (Powered by Ollama & RAG - 100% Local)") # (ØºÙŠØ±Ù†Ø§ Ø§Ù„Ø±Ø³Ø§Ù„Ø©)

# (Ø¨Ø§Ù‚ÙŠ ÙƒÙˆØ¯ Ø§Ù„Ù€ GUI Ø¨ØªØ§Ø¹Ùƒ Ø²ÙŠ Ù…Ø§ Ù‡Ùˆ... Ø§Ù„Ù€ Select Box ÙˆØ§Ù„Ù€ Expander ÙˆØ§Ù„Ø´Ø§Øª)
st.subheader("Demo: User Permission Simulation")
user_role = st.selectbox(
    "Select your role to test permissions:",
    ("Sales (Public Access)", "Product Team (Internal Access)")
)
with st.expander("Show Retrieved Context (For Demo Purposes)"):
    show_context = st.toggle("Toggle to see the context", value=False)
if "messages" not in st.session_state:
    st.session_state.messages = []
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if question := st.chat_input("What is our roadmapping process?"):

    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    with st.chat_message("assistant"):
        with st.spinner("Sage is thinking..."):

            context = retriever.invoke(question)

            if show_context:
                st.write("---")
                st.subheader("Retrieved Context:")
                st.json([doc.to_json() for doc in context])
                st.write("---")

            is_sensitive = False
            for doc in context:
                if 'project_phoenix.md' in doc.metadata.get('source', ''):
                    is_sensitive = True
                    break

            if is_sensitive and user_role == "Sales (Public Access)":
                result = "I'm sorry, I cannot answer this question as it contains information restricted to the Product Team only."
            else:
                # (Ù‡Ù†Ø§ Ø£Ù‡Ù… ØªØ¹Ø¯ÙŠÙ„: Ø´ÙŠÙ„Ù†Ø§ .content)
                result = chain.invoke({"context": context, "question": question})

            st.markdown(result)
            st.session_state.messages.append({"role": "assistant", "content": result})