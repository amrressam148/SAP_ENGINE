import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from vector import retriever 
import os 

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¸Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„ - Ø¯Ù‡ Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ø£ÙˆÙ„ Ø£Ù…Ø±) ---
st.set_page_config(page_title="Signavio Sage", page_icon="ğŸ¤–")

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¸Ø¨Ø· Ø§Ù„Ù€ API Key) ---
# (Ù„Ø§Ø²Ù… ÙŠØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù€ Key Ù…ÙˆØ¬ÙˆØ¯ Ù‚Ø¨Ù„ Ù…Ø§ ÙŠØ­Ù…Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„)
if "GOOGLE_API_KEY" not in os.environ:
    try:
        # (Ù‡Ù†Ø§ Ø¨ÙŠÙ‚Ø±Ø£ Ø§Ù„Ù€ Secret Ø§Ù„Ù„ÙŠ Ø¥Ù†Øª Ø­Ø·ÙŠØªÙ‡ ÙÙŠ Streamlit Cloud)
        os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
    except:
        st.error("GOOGLE_API_KEY not found in Streamlit Secrets!", icon="ğŸš¨")
        st.stop()

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ù€ Chain) ---
@st.cache_resource
def load_model_chain():
    """
    Loads the LLM model and the prompt chain.
    """
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.7) 

    template = """
    You are 'Signavio Sage', a helpful AI assistant for the SAP Signavio Product Excellence team.
    Your job is to answer questions about internal processes, practices, and onboarding.
    You must answer *ONLY* based on the provided context documents.

    Here is the relevant context: {context}

    Here is the question to answer: {question}

    Follow these rules:
    1. If the answer is not in the context, clearly state: 'I am sorry, I do not have information on that topic in my knowledge base.'
    2. If the answer is in the context, be concise and helpful.
    3. At the end of your answer, *always* cite the filename found in the **'source' metadata** (e.g., 'Source: onboarding.md').
    """

    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | model
    return chain

# (Ù‡Ù†Ø§ Ø¨Ù†Ø­Ù…Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆÙ†Ø·Ø¨Ø¹ Ø§Ù„Ø±Ø³Ø§Ù„Ø©)
try:
    chain = load_model_chain()
    st.success("Google Gemini model loaded successfully!") 
except Exception as e:
    st.error(f"Error loading model: {e}")
    st.stop()

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù€ GUI Ø¨ØªØ§Ø¹Ùƒ) ---
st.title("ğŸ¤– Signavio Sage Assistant")
st.info("Ask me anything... (Powered by Google Gemini & RAG)")

# (Ø§Ù„Ù€ Select Box Ø¨ØªØ§Ø¹ Ø§Ù„Ù€ Permissions)
st.subheader("Demo: User Permission Simulation")
user_role = st.selectbox(
    "Select your role to test permissions:",
    ("Sales (Public Access)", "Product Team (Internal Access)")
)

# (Ø§Ù„Ù€ Expander Ø¨ØªØ§Ø¹ Ø§Ù„Ù€ Context)
with st.expander("Show Retrieved Context (For Demo Purposes)"):
    show_context = st.toggle("Toggle to see the context", value=False)

# (ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø´Ø§Øª)
if "messages" not in st.session_state:
    st.session_state.messages = []

# (Ø¹Ø±Ø¶ Ø§Ù„Ø´Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…)
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# (Ø¯Ù‡ Ø§Ù„Ù€ input box Ø§Ù„Ù„ÙŠ ÙƒØ§Ù† Ù†Ø§Ù‚Øµ!)
if question := st.chat_input("What is our roadmapping process?"):

    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    with st.chat_message("assistant"):
        with st.spinner("Sage is thinking..."):

            context = retriever.invoke(question)

            if show_context:
                st.write("---")
                st.subheader("Retrieved Context:")
                st.json([doc.to_json() for doc in context])
                st.write("---")

            # (Ø§Ù„Ù€ Logic Ø¨ØªØ§Ø¹ Ø§Ù„Ù€ Permissions)
            is_sensitive = False
            for doc in context:
                if 'project_phoenix.md' in doc.metadata.get('source', ''):
                    is_sensitive = True
                    break

            if is_sensitive and user_role == "Sales (Public Access)":
                result = "I'm sorry, I cannot answer this question as it contains information restricted to the Product Team only."
            else:
                result_obj = chain.invoke({"context": context, "question": question})
                result = result_obj.content # (Ù„Ø§Ø²Ù… Ù†Ø²ÙˆØ¯ .content Ø¹Ø´Ø§Ù† Ù†Ø¬ÙŠØ¨ Ø§Ù„Ù€ string)

            st.markdown(result)
            st.session_state.messages.append({"role": "assistant", "content": result})