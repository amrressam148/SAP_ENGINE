import streamlit as st
import os

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¸Ø¨Ø· Ø§Ù„Ù€ API Key ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø© "Environment") ---
# Ø¯Ù‡ "Ù„Ø§Ø²Ù…" ÙŠÙƒÙˆÙ† Ø£ÙˆÙ„ Ø­Ø§Ø¬Ø© Ø®Ø§Ù„ØµØŒ Ù‚Ø¨Ù„ Ø£ÙŠ import ØªØ§Ù†ÙŠ
# Ø¹Ø´Ø§Ù† "vector.py" ÙŠÙ„Ø§Ù‚ÙŠÙ‡ Ù„Ù…Ø§ ÙŠØ´ØªØºÙ„
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("GOOGLE_API_KEY not found in Streamlit Secrets!", icon="ğŸš¨")
    st.stop()

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù€ Imports) ---
# (Ø¯Ù„ÙˆÙ‚ØªÙŠ Ù„Ù…Ø§ Ù†Ø³ØªØ¯Ø¹ÙŠ vectorØŒ Ù‡Ùˆ Ù‡ÙŠÙ„Ø§Ù‚ÙŠ Ø§Ù„Ù€ Key Ø¬Ø§Ù‡Ø²)
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from vector import retriever 

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¸Ø¨Ø· Ø§Ù„ØµÙØ­Ø©) ---
st.set_page_config(page_title="Signavio Sage", page_icon="ğŸ¤–")

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙˆØ§Ù„Ù€ Chain) ---
@st.cache_resource
def load_model_chain():
    """
    Loads the LLM model and the prompt chain.
    """
    model = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.7) 

    template = """
    You are 'Signavio Sage', a helpful AI assistant for the SAP Signavio Product Excellence team.
    Your job is to answer questions about internal processes, practices, and onboarding.
    You must answer *ONLY* based on the provided context documents.

    Here is the relevant context: {context}

    Here is the question to answer: {question}

    Follow these rules:
    1. If the answer is not in the context, clearly state: 'I am sorry, I do not have information on that topic in my knowledge base.'
    2. If the answer is in the context, be concise and helpful.
    3. At the end of your answer, *always* cite the filename found in the **'source' metadata** (e.g., 'Source: onboarding.md').
    """

    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | model
    return chain

try:
    chain = load_model_chain()
    st.success("Google Gemini model loaded successfully!") 
except Exception as e:
    st.error(f"Error loading model: {e}")
    st.stop()

# --- (Ø§Ù„Ø®Ø·ÙˆØ© 5: Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù€ GUI Ø¨ØªØ§Ø¹Ùƒ - Ø²ÙŠ Ù…Ø§ Ù‡Ùˆ Ø¨Ø§Ù„Ø¸Ø¨Ø·) ---
st.title("ğŸ¤– Signavio Sage Assistant")
st.info("Ask me anything... (Powered by Google Gemini & RAG)")

st.subheader("Demo: User Permission Simulation")
user_role = st.selectbox(
    "Select your role to test permissions:",
    ("Sales (Public Access)", "Product Team (Internal Access)")
)

with st.expander("Show Retrieved Context (For Demo Purposes)"):
    show_context = st.toggle("Toggle to see the context", value=False)

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if question := st.chat_input("What is our roadmapping process?"):

    st.session_state.messages.append({"role": "user", "content": question})
    with st.chat_message("user"):
        st.markdown(question)

    with st.chat_message("assistant"):
        with st.spinner("Sage is thinking..."):

            context = retriever.invoke(question)

            if show_context:
                st.write("---")
                st.subheader("Retrieved Context:")
                st.json([doc.to_json() for doc in context])
                st.write("---")

            is_sensitive = False
            for doc in context:
                if 'project_phoenix.md' in doc.metadata.get('source', ''):
                    is_sensitive = True
                    break

            if is_sensitive and user_role == "Sales (Public Access)":
                result = "I'm sorry, I cannot answer this question as it contains information restricted to the Product Team only."
            else:
                result_obj = chain.invoke({"context": context, "question": question})
                result = result_obj.content 

            st.markdown(result)
            st.session_state.messages.append({"role": "assistant", "content": result})